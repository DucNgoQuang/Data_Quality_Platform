
services:
  airflow-db:
    image: postgres
    container_name: airflow-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      timeout: 3s
      retries: 3
    networks:
      - lakehouse_net
    volumes:
      - ./airflow/db:/var/lib/postgresql/data

  airflow:
    container_name: airflow
    build:
      context: ./airflow
      dockerfile: Dockerfile
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    depends_on:
      airflow-db:
        condition: service_healthy
    ports:
      - 8080:8080
    networks:
      - lakehouse_net
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/jobs:/opt/airflow/jobs
      - ./dqops/volume:/opt/dqops/volume
    command: >
      bash -c "
        airflow db upgrade ;
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com ;
        airflow webserver -p 8080 &
        airflow scheduler ;
      "
  minio:
    image: minio/minio
    container_name: minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_DOMAIN=minio
    networks:
      lakehouse_net:
        aliases:
          - warehouse.minio
    volumes:
      - ./minio/data:/data
    ports:
      - 9001:9001
      - 9000:9000
    command: ["server", "/data", "--console-address", ":9001"]
  mc:
    depends_on:
      - minio
    image: minio/mc
    container_name: mc
    networks:
      lakehouse_net:
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc alias set minio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc rm -r --force minio/warehouse;
      /usr/bin/mc mb minio/warehouse;
      /usr/bin/mc policy set public minio/warehouse;
      tail -f /dev/null
      "      
  superset:
    image: superset
    build: ./superset
    container_name: superset
    environment:
      - SUPERSET_SECRET_KEY=secret
    ports:
      - 8088:8088
    networks:
      lakehouse_net:
    depends_on:
      - spark-thrift
    volumes:
      - ./superset/data:/app/superset_home
      - ./superset/config.py:/app/superset/config.py
    entrypoint: >
      /bin/sh -c "
      superset fab create-admin --username admin --firstname superset --lastname admin --email admin@superset.com --password admin;
      superset db upgrade;
      superset init;
      superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger;
      "      
  metastore_db:
    image: postgres
    container_name: metastore_db
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
    networks:
      - lakehouse_net
    volumes:
      - ./metastore_db:/var/lib/postgresql/data
  hive-metastore:
    image: naushadh/hive-metastore
    container_name: hive-metastore
    depends_on:
      - metastore_db
      - minio
    environment:
      - DATABASE_HOST=metastore_db
      - DATABASE_DB=metastore
      - DATABASE_USER=hive
      - DATABASE_PASSWORD=hive
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - S3_ENDPOINT_URL=http://minio:9000
      - S3_BUCKET=warehouse
      - S3_PREFIX='hive'
    ports:
      - 9083:9083
    networks:
      - lakehouse_net
  spark-thrift:
    container_name: spark-thrift
    image: spark:3.5.6-scala2.12-java17-python3-ubuntu
    entrypoint: ['./entrypoint.sh', 'thrift' ]
    #entrypoint: ["bash", "-c", "tail -f /dev/null"]
    volumes:
      - ./spark/entrypoint.sh:/opt/spark/work-dir/entrypoint.sh
      - ./spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./data:/opt/spark/data
      - spark-logs:/opt/spark/spark-events
    ports:
      - 10000:10000
    depends_on:
      - hive-metastore
      - minio
    networks:
      - lakehouse_net

  spark-master:
    container_name: spark-master
    image: spark:3.5.6-scala2.12-java17-python3-ubuntu
    entrypoint: ['./entrypoint.sh', 'master']
    #entrypoint: ["bash", "-c", "tail -f /dev/null"]
    volumes:
      - ./spark/entrypoint.sh:/opt/spark/work-dir/entrypoint.sh
      - ./spark/jobs:/opt/spark/work-dir/jobs
      - ./data:/opt/spark/data
      - ./dqops/volume:/opt/dqops/volume
      - spark-logs:/opt/spark/spark-events
    ports:
      - '8081:8080'
      - '7077:7077'
    networks:
      - lakehouse_net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 5s
      timeout: 3s
      retries: 3

  spark-worker-1:
    container_name: spark-worker-1
    image: spark:3.5.6-scala2.12-java17-python3-ubuntu
    entrypoint: ['./entrypoint.sh', 'worker']
    depends_on:
      - spark-master
    volumes:
      - ./spark/entrypoint.sh:/opt/spark/work-dir/entrypoint.sh
      - ./data:/opt/spark/data
      - spark-logs:/opt/spark/spark-events
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1g
    networks:
      - lakehouse_net

#   spark-history-server:
#     container_name: spark-history-server
#     image: spark:3.5.6-scala2.12-java17-python3-ubuntu
#     entrypoint: ['./entrypoint.sh', 'history']
#     depends_on:
#       - spark-master
#     volumes:
#       - ./spark/entrypoint.sh:/opt/spark/work-dir/entrypoint.sh
#       - spark-logs:/opt/spark/spark-events
#     ports:
#       - '18080:18080'
#     networks:
#       - spark_network

networks:
  lakehouse_net:
    external: true

volumes:
  spark-logs:
    driver: local
  metastore_db:
    driver: local