{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b0ab1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark\n",
    "from datetime import datetime\n",
    "import os\n",
    "from delta import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5f2ffec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now()\n",
    "current_month_partition = current_date.strftime(\"m=%Y-%m-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6f8b1868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-06-14'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_date = current_date.strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d971acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQOPS_HOME_DATA_PATH = '/opt/dqops/volume/.data/check_results'\n",
    "MINIO_ENDPOINT = 'http://localhost:9000'\n",
    "MINIO_ACCESS_KEY = 'admin'\n",
    "MINIO_SECRET_ACCESS_KEY = 'password'\n",
    "AWS_REGION = 'us-east-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "18f9ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.conf.SparkConf()\n",
    "    .setAppName(\"MY_APP\")\n",
    "    .set(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_ACCESS_KEY)\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \n",
    "    .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")  \n",
    "    .set(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\")\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .setMaster(\"local[*]\")\n",
    ")\n",
    "\n",
    "extra_packages = [\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"org.apache.hadoop:hadoop-common:3.3.4\",\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "dd833808",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\").config(conf=conf)\n",
    "spark = configure_spark_with_delta_pip(\n",
    "    builder, extra_packages=extra_packages\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e0044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_check_results(file_path):\n",
    "    df = spark.read.parquet(file_path)\n",
    "\n",
    "    df = df.where(F.to_date('executed_at') == F.lit(current_date))\n",
    "\n",
    "    df = df.withColumn('severity' , F.when(F.col('severity') == 0 , 'correct') \\\n",
    "                                .when(F.col('severity') == 1, 'warning') \\\n",
    "                                .when(F.col('severity') == 2, 'error') \\\n",
    "                                .otherwise('fatal')) \n",
    "\n",
    "    df.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .save(f\"s3a://dqops/check_results\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "13230c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for c_dir in os.listdir(DQOPS_HOME_DATA_PATH):\n",
    "    c_path = os.path.join(DQOPS_HOME_DATA_PATH, c_dir)\n",
    "\n",
    "    for t_dir in os.listdir(c_path):\n",
    "        t_path = os.path.join(c_path, t_dir)\n",
    "        month_path = os.path.join(t_path, current_month_partition)\n",
    "\n",
    "        if os.path.isdir(month_path):\n",
    "            parquet_file_path = os.path.join(month_path, \"check_results.0.parquet\")\n",
    "            preprocess_check_results(parquet_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67e06c",
   "metadata": {},
   "source": [
    "severity = 0: Typically means the check passed or is \"correct\" (no issues found, or the actual_value is within the expected/acceptable bounds).\n",
    "severity = 1: Indicates a warning. The actual_value is outside the warning_lower_bound or warning_upper_bound, but not yet in the error or fatal range.\n",
    "severity = 2: Indicates an error. The actual_value is outside the error_lower_bound or error_upper_bound.\n",
    "severity = 3: Indicates a fatal issue. The actual_value is outside the fatal_lower_bound or fatal_upper_bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5319b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(f\"s3a://dqops/check_results\")\n",
    "\n",
    "df = df['executed_at']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
